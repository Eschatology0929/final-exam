{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil                           # 移动文件\n",
    "import random                           # 随机化抽取文件\n",
    "import numpy as np                      # 画图\n",
    "import matplotlib.pyplot as plt         # 画图\n",
    "from nltk.corpus import stopwords       # 去停用词\n",
    "\n",
    "cachedStopWords = stopwords.words(\"english\")    # 选用英文停用词词典\n",
    "\n",
    "def fileWalker(path):\n",
    "    # 遍历语料目录，将所有语料文件绝对路径存入列表fileArray\n",
    "    fileArray = []\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for fn in files:\n",
    "            eachpath = str(root+'\\\\'+fn)\n",
    "            fileArray.append(eachpath)\n",
    "    return fileArray\n",
    "\n",
    "\n",
    "def test_set_select():\n",
    "    # 从spam和ham集中随机选10封移动到test集中作为测试集\n",
    "    filepath = r'..\\email'\n",
    "    testpath = r'..\\email\\test'\n",
    "    files = fileWalker(filepath)\n",
    "    random.shuffle(files)\n",
    "    top10 = files[:10]\n",
    "    for ech in top10:\n",
    "        ech_name = testpath+'\\\\'+('_'.join(ech.split('\\\\')[-2:]))  # 取分割后的后两项用_拼接\n",
    "        shutil.move(ech, testpath)  # 把ech移动到testpath文件夹下\n",
    "        os.rename(testpath+'\\\\'+ech.split('\\\\')[-1], ech_name)  # 把ech更名为ech_name,其实可以和上一步合并\n",
    "        # print('%s moved' % ech_name)\n",
    "    return\n",
    "\n",
    "\n",
    "def test_set_clear():\n",
    "    # 移动test测试集中文件回spam和ham中，等待重新抽取测试集\n",
    "    filepath = r'..\\email'\n",
    "    testpath = r'..\\email\\test'\n",
    "    files = fileWalker(testpath)\n",
    "    for ech in files:\n",
    "        ech_initial = filepath + '\\\\' + '\\\\'.join(' '.join(ech.split('\\\\')[-1:]).split('_'))  # 分析出文件移入测试集前的目录及名称\n",
    "        ech_move = filepath + '\\\\' + (' '.join(ech.split('\\\\')[-1:]).split('_'))[0]  # 分析出文件移入测试集前的目录\n",
    "        shutil.move(ech, ech_move)  # 把ech移动到ech_move文件夹下\n",
    "        os.rename(ech_move+'\\\\'+' '.join(ech.split('\\\\')[-1:]), ech_initial)  # 恢复原名称\n",
    "        # print('%s moved' % ech)\n",
    "    return\n",
    "\n",
    "\n",
    "def readtxt(path, encoding):\n",
    "    # 按encoding方式按行读取path路径文件所有行，返回行列表lines\n",
    "    with open(path, 'r', encoding=encoding) as f:\n",
    "        lines = f.readlines()\n",
    "    return lines\n",
    "\n",
    "def fileWalker(path):\n",
    "    # 获取path路径下所有文件的绝对路径列表fileArray\n",
    "    fileArray = []\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for fn in files:\n",
    "            eachpath = str(root+'\\\\'+fn)\n",
    "            fileArray.append(eachpath)\n",
    "    return fileArray\n",
    "\n",
    "def email_parser(email_path):\n",
    "    # 去特殊字符标点符号，返回纯单词列表clean_word\n",
    "    punctuations = \"\"\",.<>()*&^%$#@!'\";~`[]{}|、\\\\/~+_-=?\"\"\"\n",
    "    content_list = readtxt(email_path, 'gbk')\n",
    "    content = (' '.join(content_list)).replace('\\r\\n', ' ').replace('\\t', ' ')\n",
    "    clean_word = []\n",
    "    for punctuation in punctuations:\n",
    "        content = (' '.join(content.split(punctuation))).replace('  ', ' ')\n",
    "        clean_word = [word.lower()\n",
    "                      for word in content.split(' ') if word.lower() not in cachedStopWords and len(word) > 2]\n",
    "        # 此处去了停用词，可不去，影响不大\n",
    "    return clean_word\n",
    "\n",
    "\n",
    "def get_word(email_file):\n",
    "    # 获取email_file路径下所有文件的总单词列表，append入word_list，extend入word_set并去重转为set\n",
    "    word_list = []\n",
    "    word_set = []\n",
    "    email_paths = fileWalker(email_file)\n",
    "    for email_path in email_paths:\n",
    "        clean_word = email_parser(email_path)\n",
    "        word_list.append(clean_word)\n",
    "        word_set.extend(clean_word)\n",
    "        # print(set(word_set))\n",
    "    return word_list, set(word_set)\n",
    "\n",
    "\n",
    "def count_word_prob(email_list, union_set):\n",
    "    # 返回训练集词频字典word_prob\n",
    "    word_prob = {}\n",
    "    for word in union_set:\n",
    "        counter = 0\n",
    "        for email in email_list:\n",
    "            if word in email:\n",
    "                counter += 1\n",
    "            else:\n",
    "                continue\n",
    "        prob = 0.0\n",
    "        if counter != 0:\n",
    "            prob = counter/len(email_list)\n",
    "        else:\n",
    "            prob = 0.05  # 进在某一分类中未出现则令该分类下该词词频TF=0.01，0.05，……，越大越会把spam误判成ham\n",
    "        word_prob[word] = prob\n",
    "    return word_prob\n",
    "\n",
    "\n",
    "def filter(ham_word_pro, spam_word_pro, test_file):\n",
    "    # 进行一次对测试集(10封邮件)的测试，输出对测试集的判断结果\n",
    "    # 并返回准确率right_rate，以及把spam误判成ham和总误判次数对应情况\n",
    "    right = 0\n",
    "    wrong = 0\n",
    "    wrong_spam = 0\n",
    "    test_paths = fileWalker(test_file)\n",
    "    for test_path in test_paths:\n",
    "        # 贝叶斯推断计算与判别实现\n",
    "        email_spam_prob = 0.0\n",
    "        spam_prob = 0.5  # 假设P(spam) = 0.5\n",
    "        ham_prob = 0.5  # P(ham) = 0.5\n",
    "        file_name = test_path.split('\\\\')[-1]\n",
    "        prob_dict = {}\n",
    "        words = set(email_parser(test_path))\n",
    "        for word in words:  # 统计测试集所出现单词word的P(spam|word)\n",
    "            Psw = 0.0\n",
    "            if word not in spam_word_pro:\n",
    "                Psw = 0.4  # 第一次出现的新单词设P(spam|new word) = 0.4 by Paul Graham\n",
    "            else:\n",
    "                Pws = spam_word_pro[word]  # P(word|spam)\n",
    "                Pwh = ham_word_pro[word]  # P(word|ham)\n",
    "                Psw = spam_prob*(Pws/(Pwh*ham_prob+Pws*spam_prob))\n",
    "                # P(spam|word) = P(spam)*P(word|spam)/P(word)\n",
    "                #              = P(spam)*P(word|spam)/(P(word|ham)*P(ham)+P(word|spam)*P(spam))\n",
    "            prob_dict[word] = Psw\n",
    "        numerator = 1\n",
    "        denominator_h = 1\n",
    "        for k, v in prob_dict.items():\n",
    "            numerator *= v  # P1P2…Pn = P(spam|word1)*P(spam|word2)*…*P(spam|wordn)\n",
    "            denominator_h *= (1-v)  # (1-P1)(1-P2)…(1-Pn) = (1-P(spam|word1))*(1-P(spam|word2))*…*(1-P(spam|wordn))\n",
    "        email_spam_prob = round(numerator/(numerator+denominator_h), 4)\n",
    "        # P(spam|word1word2…wordn) = P1P2…Pn/(P1P2…Pn+(1-P1)(1-P2)…(1-Pn))\n",
    "\n",
    "        if email_spam_prob > 0.9:  # P(spam|word1word2…wordn) > 0.9 认为是spam垃圾邮件\n",
    "            print(file_name, 'spam', email_spam_prob)\n",
    "            if file_name.split('_')[1] == '25.txt':\n",
    "                print(prob_dict)\n",
    "            if file_name.split('_')[0] == 'spam':  # 记录是否判断准确\n",
    "                right += 1\n",
    "            else:\n",
    "                wrong += 1\n",
    "                print('***********************Wrong Prediction***********************')\n",
    "        else:\n",
    "            print(file_name, 'ham', email_spam_prob)\n",
    "            if file_name.split('_')[1] == '25.txt':\n",
    "                print(prob_dict)\n",
    "            if file_name.split('_')[0] == 'ham':  # 记录是否判断准确\n",
    "                right += 1\n",
    "            else:\n",
    "                wrong += 1\n",
    "                wrong_spam += 1  # 记录把spam误判成ham的次数\n",
    "                print('***********************Wrong Prediction***********************')\n",
    "\n",
    "        # print(prob_dict)\n",
    "    right_rate = right/(right+wrong)  # 计算一个测试集的准确率\n",
    "    if wrong != 0:\n",
    "        wrong_spam_rate = [wrong_spam, wrong]  # [把spam误判成ham的次数，总误判次数]\n",
    "    else:\n",
    "        wrong_spam_rate = [-1]  # 表示总误判次数为0\n",
    "    return right_rate, wrong_spam_rate\n",
    "\n",
    "\n",
    "def main():\n",
    "    # 主函数\n",
    "    right_rate_list = []\n",
    "    wrong_spam_rate_list = []\n",
    "    ham_file = r'..\\email\\ham'\n",
    "    spam_file = r'..\\email\\spam'\n",
    "    test_file = r'..\\email\\test'\n",
    "    for i in range(100):\n",
    "        # 进行100次抽取测试集，测试并记录准确率，注意训练集应不包含测试集\n",
    "        test_set_select()  # 构造测试集\n",
    "        ham_list, ham_set = get_word(ham_file)\n",
    "        spam_list, spam_set = get_word(spam_file)\n",
    "        union_set = ham_set | spam_set  # 合并纯单词集合\n",
    "        ham_word_pro = count_word_prob(ham_list, union_set)  # 单词在ham中的出现频率字典\n",
    "        spam_word_pro = count_word_prob(spam_list, union_set)  # 单词在spam里的出现频率字典\n",
    "        rig, wrg = filter(ham_word_pro, spam_word_pro, test_file)\n",
    "        right_rate_list.append(rig)  # 返回正确率\n",
    "        wrong_spam_rate_list.append(wrg)  # 返回误报spam->ham占比\n",
    "        test_set_clear()  # 还原测试集\n",
    "    # 画出100次判别的准确率散点图\n",
    "    x = range(100)\n",
    "    y = right_rate_list\n",
    "    plt.scatter(x, y)\n",
    "    plt.title('Correct Rate of 100 Times')\n",
    "    plt.show()\n",
    "    # 输出100次误报spam->ham占比列表\n",
    "    print(wrong_spam_rate_list)\n",
    "    return\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3 (default, Jul  2 2020, 17:30:36) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cf8b08f5ab85e7c0fc5d10dd2633a4d50f51dec650140fc8f8406da16eba4529"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
